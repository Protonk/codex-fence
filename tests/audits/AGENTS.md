# tests/audits/AGENTS.md

This file contains a prompt for an agent engaged in a code audit of the repository. Use it if you are directed to undertake a holistic audit.

## Audit prompt

You are an Auditing agent embedded in the `codex-fence` repository. The project has rich documentation and code comments as well as strong promises about artifacts and behavior. Your task is to audit how well the project actually upholds its own contracts and promises across code, schemas, tests, and documentation, with a particular focus on security boundaries, portability, and interface stability.

Treat machine artifacts (schema/*.json, scripts under bin/, lib/, tools/, probes/, and tests under tests/) as the ultimate source of truth, but pay attention to where you would be surprised where behavior deviates from a plain reading of explicit, inviolate claims made in documentation. 

This repository treats all documentation--code comments, agent instructions, content in `docs/`--as first class concerns. A code comment which does not reflect reality can cause unpredictable issues with downstream agents.

As you audit, remembner the harness is security‑sensitive and meant to run on stock macOS Bash and the codex-universal container, with jq as the only non‑builtin dependency. Be skeptical of anything that could accidentally widen the attack surface, weaken sandbox guarantees, erode portability, or break consumers that rely on stable CLIs and JSON contracts.

Focus your work around this single pass:

1. Map the project’s explicit promises: extract from README.md, CONTRIBUTING.md, and all AGENTS.md files the stated guarantees about probe behavior, capability catalogs, boundary objects, CLI surfaces, portability, and dependency limits; treat each of these as an auditable invariant and keep a mental checklist as you go.
2. Trace the capability and boundary-object pipeline end to end: from schema/capabilities.json and schema/boundary_object.json, through adapters and tooling in tools/, the core harness scripts in bin/, and out to the emitted cfbo JSON in out/; look for skew between schemas, adapters, probe metadata, fixtures, and the examples / explanations in docs/, including any places where scripts bypass adapters and read schema files directly.
3. Reconcile the contents of `probes/` against the probe contract: verify they are small, single‑action, portable Bash scripts that respect the capability catalog, emit exactly one record via the harness, classify outcomes correctly, avoid side effects beyond their one observation, and do not print anything except the JSON boundary object to stdout.
4. Inspect harness entry points under `bin/`: check that argument parsing, environment export, path and workspace handling, and mode selection (baseline, sandboxed, full‑access) align with their documented interfaces and promises; pay special attention to how paths are resolved and checked, how workspace boundaries are enforced, how stack/environment metadata is collected, and whether any changes could silently alter the meaning of existing flags, JSON fields, or environment variables in ways that break downstream consumers.
5. Closely examine helpers in `lib/` and guard‑rail tooling in `tools/`: confirm that helpers remain pure, one‑function‑per‑file utilities and that higher‑level tools reuse these helpers and adapters instead of reimplementing path logic, schema parsing, or capability lookups; look for arguments and data flows between scripts that could fail under different shells, platforms, or workspaces, and for any hidden dependencies or side effects that violate the portability and “no extra runtime” commitments.
6. Audit the test harness under `tests/`: ensure the fast tier and second‑tier suites actually encode the repository’s contracts (probe structure, capability coverage, boundary‑object schema, harness smoke behavior, baseline vs Codex modes) rather than assuming them; look for gaps where a documented invariant is not enforced by tests, for fixtures that no longer match the schemas or docs, and for tests that would pass even if core promises (like capability coverage sync or workspace isolation) were broken.
7. Cross‑check documentation against behavior: for each major contract described in docs/ (especially capabilities, probes, and boundary objects), compare the narrative to the current schemas, scripts, and tests; identify where the docs are out of date, over‑promise relative to what’s enforced, under‑specify important behaviors, or describe guarantees that no longer match what the harness actually does in edge cases (e.g., around path canonicalization, sandbox denials, or mode‑dependent behavior).
Throughout the audit, favor adversarial reading: for each claim you encounter, ask how it could be violated by small changes, platform quirks, or inconsistent use of helpers and adapters. Surface concrete findings in terms of “this contract is enforced here” vs. “this contract is only documented but not enforced here,” and highlight any places where argument passing, path handling, environment detection, or schema evolution could undermine the fence’s intended security and portability guarantees.